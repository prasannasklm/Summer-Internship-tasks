{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summer_Intership_Task1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw867REc3axZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d85d9c18-8e99-4916-b569-1356e2fe8ed1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/Colab Notebooks\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxTzqlbOj5LC",
        "outputId": "320ffb63-dd08-44ff-ee94-c6c2cbecbba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Copy of SummerIntern1.ipynb'   telugu_test.txt\n",
            "'Copy of Untitled3.ipynb'       telugu_train.txt\n",
            " dev.europarl\t\t        test.europarl\n",
            " dev.telugu.txt\t\t        test.telugu.txt\n",
            " english_test.txt\t        train.europarl\n",
            " english_train.txt\t        train.telugu.txt\n",
            "'Language Model.ipynb'\t        Untitled0.ipynb\n",
            " LM_English_test.txt\t        Untitled1.ipynb\n",
            " LM_English_train.txt\t        Untitled2.ipynb\n",
            " report1.txt\t\t        Untitled3.ipynb\n",
            " report.txt\t\t       'Week-1_Lecture (1).ipynb'\n",
            "'SummerIntern1 (1).ipynb'       Week-1_Lecture.ipynb\n",
            "'SummerIntern1 (2).ipynb'      'Week-2 Lecture.ipynb'\n",
            "'SummerIntern1 (3).ipynb'      'Week-3 Lecture.ipynb'\n",
            " SummerIntern1.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "nlp = spacy.load(name='en_core_web_sm')\n",
        "def counts(str):\n",
        "  cou = 0\n",
        "  for sent in new_sentences:\n",
        "    cou +=sent.count(str)\n",
        "  return cou"
      ],
      "metadata": {
        "id": "ZFAVrhr3kBjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "BnqwZCCYkItS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/My Drive/Colab Notebooks/train.europarl\",\"r\")\n",
        "sentences =(file.readlines())\n",
        "#REMOVED PUNCTUATIONS\n",
        "new_sentences=[]\n",
        "for sent in sentences:\n",
        "  new_sentences.append(re.sub(r'[^\\w\\s]', '',sent))"
      ],
      "metadata": {
        "id": "v1CXVhXjkNQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prob(tri):\n",
        "  string1 = tri[0]+\" \"+tri[1]+\" \"+tri[2]\n",
        "  string2 = tri[0]+\" \"+tri[1]\n",
        "  count1=counts(string1)\n",
        "  count2=counts(string2)\n",
        "  #print(string1,\":\",count1)\n",
        "  #print(string2,\":\",count2)\n",
        "  if(count2==0 or count1==0):\n",
        "    return 0.01\n",
        "  return count1/count2\n",
        "\n",
        "\n",
        "#KNEYSER NEY SMOOTHING\n",
        "\n",
        "def d_value():\n",
        "  #Here need to find absolute discount value..\n",
        "  return 0.75\n",
        "\n",
        "def kneyserNey(trigram):\n",
        "  string1 = trigram[0]+\" \"+trigram[1]+\" \"+trigram[2]\n",
        "  string2 = trigram[0]+\" \"+trigram[1]\n",
        "  count1=counts(string1)\n",
        "  count2=counts(string2)\n",
        "  if(count2==0):\n",
        "    count2=0.01\n",
        "\n",
        "  #lets find the first term of the formula\n",
        "  d = d_value()\n",
        "  C_kn = max(count1-d , 0) / count2\n",
        "  lamda = d / (count2)*1\n",
        "  p_cont = counts(trigram[2]) / len(sentences)\n",
        "\n",
        "  p_trigram = C_kn+ lamda*p_cont\n",
        "  return p_trigram"
      ],
      "metadata": {
        "id": "EI1-8jzPkVjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Data"
      ],
      "metadata": {
        "id": "KmGOah9MkgV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/My Drive/Colab Notebooks/test.europarl\",\"r\")\n",
        "sentences =(file.readlines())\n",
        "#REMOVED PUNCTUATIONS\n",
        "new_sentences=[]\n",
        "for sent in sentences:\n",
        "  new_sentences.append(re.sub(r'[^\\w\\s]', '',sent))"
      ],
      "metadata": {
        "id": "xC7KRZqykkys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dev Data"
      ],
      "metadata": {
        "id": "afqO5Q7Mk1cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/My Drive/Colab Notebooks/dev.europarl\",\"r\")\n",
        "sentences =(file.readlines())\n",
        "#REMOVED PUNCTUATIONS\n",
        "new_sentences=[]\n",
        "for sent in sentences:\n",
        "  new_sentences.append(re.sub(r'[^\\w\\s]', '',sent))"
      ],
      "metadata": {
        "id": "-qGfNJQfk63W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model For Finding Train Split Sentences Probabilities"
      ],
      "metadata": {
        "id": "j9VYfJxolAEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TRAIN DATA MODEL FOR PROBABILITIES\n",
        "# Create a placeholder for model\n",
        "model ={}\n",
        "LM_English_train = open(\"/content/drive/My Drive/Colab Notebooks/LM_English_train.txt\",\"w\")\n",
        "LM_English_train.write(\"\\n\\n\\t\\t\\tENGLISH TRAIN SPLIT SENTANCES PROBABILITIES\\n\\n\")\n",
        "for i in range(len(new_sentences)):\n",
        "  sent = new_sentences[i][:-1]\n",
        "  doc = nlp(sent)\n",
        "  tokens=[token.text for token in doc]\n",
        "  #GENERATING EACH SENTENCE TRIAGRAMS and FINDING PROBABILITIES\n",
        "  probability={}\n",
        "  for j in range(3,len(tokens)+1):\n",
        "    key = tokens[j-3]+\" \"+tokens[j-2]+\" \"+tokens[j-1]\n",
        "    probability[key]=prob(tokens[j-3:j])\n",
        "  sent_prob=1\n",
        "  print(i,end=\" ,\")\n",
        "  for probs in probability:\n",
        "    sent_prob *=probability[probs]\n",
        "  model[sentences[i]]=sent_prob\n",
        "  LM_English_train.write(sentences[i]+\"\\t\"+str(sent_prob)+\"\\n\")\n",
        "\n",
        "#find average probability\n",
        "total_prob = 0\n",
        "for sent_prob in model:\n",
        "  total_prob += model[sent_prob]\n",
        "total_prob= total_prob/len(new_sentences)\n",
        "\n",
        "#WRITING AVERAGE PROBABILITY\n",
        "LM_English_train.write(\"\\navg_probability \"+\" : \"+str(total_prob)+\"\\n\")  \n",
        "LM_English_train.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgNs46StlI8M",
        "outputId": "ca2b2389-5a7c-40db-86a1-1f067a7213fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 ,1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ,10 ,11 ,12 ,13 ,14 ,15 ,16 ,17 ,18 ,19 ,20 ,21 ,22 ,23 ,24 ,25 ,26 ,27 ,28 ,29 ,30 ,31 ,32 ,33 ,34 ,35 ,36 ,37 ,38 ,39 ,40 ,41 ,42 ,43 ,44 ,45 ,46 ,47 ,48 ,49 ,50 ,51 ,52 ,53 ,54 ,55 ,56 ,57 ,58 ,59 ,60 ,61 ,62 ,63 ,64 ,65 ,66 ,67 ,68 ,69 ,70 ,71 ,72 ,73 ,74 ,75 ,76 ,77 ,78 ,79 ,80 ,81 ,82 ,83 ,84 ,85 ,86 ,87 ,88 ,89 ,90 ,91 ,92 ,93 ,94 ,95 ,96 ,97 ,98 ,99 ,100 ,101 ,102 ,103 ,104 ,105 ,106 ,107 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,120 ,121 ,122 ,123 ,124 ,125 ,126 ,127 ,128 ,129 ,130 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,146 ,147 ,148 ,149 ,150 ,151 ,152 ,153 ,154 ,155 ,156 ,157 ,158 ,159 ,160 ,161 ,162 ,163 ,164 ,165 ,166 ,167 ,168 ,169 ,170 ,171 ,172 ,173 ,174 ,175 ,176 ,177 ,178 ,179 ,180 ,181 ,182 ,183 ,184 ,185 ,186 ,187 ,188 ,189 ,190 ,191 ,192 ,193 ,194 ,195 ,196 ,197 ,198 ,199 ,200 ,201 ,202 ,203 ,204 ,205 ,206 ,207 ,208 ,209 ,210 ,211 ,212 ,213 ,214 ,215 ,216 ,217 ,218 ,219 ,220 ,221 ,222 ,223 ,224 ,225 ,226 ,227 ,228 ,229 ,230 ,231 ,232 ,233 ,234 ,235 ,236 ,237 ,238 ,239 ,240 ,241 ,242 ,243 ,244 ,245 ,246 ,247 ,248 ,249 ,250 ,251 ,252 ,253 ,254 ,255 ,256 ,257 ,258 ,259 ,260 ,261 ,262 ,263 ,264 ,265 ,266 ,267 ,268 ,269 ,270 ,271 ,272 ,273 ,274 ,275 ,276 ,277 ,278 ,279 ,280 ,281 ,282 ,283 ,284 ,285 ,286 ,287 ,288 ,289 ,290 ,291 ,292 ,293 ,294 ,295 ,296 ,297 ,298 ,299 ,300 ,301 ,302 ,303 ,304 ,305 ,306 ,307 ,308 ,309 ,310 ,311 ,312 ,313 ,314 ,315 ,316 ,317 ,318 ,319 ,320 ,321 ,322 ,323 ,324 ,325 ,326 ,327 ,328 ,329 ,330 ,331 ,332 ,333 ,334 ,335 ,336 ,337 ,338 ,339 ,340 ,341 ,342 ,343 ,344 ,345 ,346 ,347 ,348 ,349 ,350 ,351 ,352 ,353 ,354 ,355 ,356 ,357 ,358 ,359 ,360 ,361 ,362 ,363 ,364 ,365 ,366 ,367 ,368 ,369 ,370 ,371 ,372 ,373 ,374 ,375 ,376 ,377 ,378 ,379 ,380 ,381 ,382 ,383 ,384 ,385 ,386 ,387 ,388 ,389 ,390 ,391 ,392 ,393 ,394 ,395 ,396 ,397 ,398 ,399 ,400 ,401 ,402 ,403 ,404 ,405 ,406 ,407 ,408 ,409 ,410 ,411 ,412 ,413 ,414 ,415 ,416 ,417 ,418 ,419 ,420 ,421 ,422 ,423 ,424 ,425 ,426 ,427 ,428 ,429 ,430 ,431 ,432 ,433 ,434 ,435 ,436 ,437 ,438 ,439 ,440 ,441 ,442 ,443 ,444 ,445 ,446 ,447 ,448 ,449 ,450 ,451 ,452 ,453 ,454 ,455 ,456 ,457 ,458 ,459 ,460 ,461 ,462 ,463 ,464 ,465 ,466 ,467 ,468 ,469 ,470 ,471 ,472 ,473 ,474 ,475 ,476 ,477 ,478 ,479 ,480 ,481 ,482 ,483 ,484 ,485 ,486 ,487 ,488 ,489 ,490 ,491 ,492 ,493 ,494 ,495 ,496 ,497 ,498 ,499 ,"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model For Finding probabilities of Test Split Sentences"
      ],
      "metadata": {
        "id": "cAXBPA-Glla7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TRAIN DATA MODEL FOR PROBABILITIES\n",
        "# Create a placeholder for model\n",
        "model ={}\n",
        "LM_English_test = open(\"/content/drive/My Drive/Colab Notebooks/LM_English_test.txt\",\"w\")\n",
        "LM_English_test.write(\"\\n\\n\\t\\t\\tENGLISH TEST SPLIT SENTANCES PROBABILITIES\\n\\n\")\n",
        "for i in range(len(new_sentences)):\n",
        "  sent = new_sentences[i][:-1]\n",
        "  doc = nlp(sent)\n",
        "  tokens=[token.text for token in doc]\n",
        "  #GENERATING EACH SENTENCE TRIAGRAMS and FINDING PROBABILITIES\n",
        "  probability={}\n",
        "  for j in range(3,len(tokens)+1):\n",
        "    key = tokens[j-3]+\" \"+tokens[j-2]+\" \"+tokens[j-1]\n",
        "    probability[key]=prob(tokens[j-3:j])\n",
        "  sent_prob=1\n",
        "  print(i,end=\" ,\")\n",
        "  for probs in probability:\n",
        "    sent_prob *=probability[probs]\n",
        "  model[sentences[i]]=sent_prob\n",
        "  LM_English_test.write(sentences[i]+\"\\t\"+str(sent_prob)+\"\\n\")\n",
        "\n",
        "#find average probability\n",
        "total_prob = 0\n",
        "for sent_prob in model:\n",
        "  total_prob += model[sent_prob]\n",
        "total_prob= total_prob/len(new_sentences)\n",
        "\n",
        "#WRITING AVERAGE PROBABILITY\n",
        "LM_English_test.write(\"\\navg_probability \"+\" : \"+str(total_prob)+\"\\n\")\n",
        "LM_English_test.write(\"performance \"+\" : \"+str(total_prob**(-1/3))+\"\\n\")  \n",
        "LM_English_test.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgif6Ei1l3yp",
        "outputId": "9e1207b5-fca8-4191-c0a5-b68dfaa98807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 ,1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ,10 ,11 ,12 ,13 ,14 ,15 ,16 ,17 ,18 ,19 ,20 ,21 ,22 ,23 ,24 ,25 ,26 ,27 ,28 ,29 ,30 ,31 ,32 ,33 ,34 ,35 ,36 ,37 ,38 ,39 ,40 ,41 ,42 ,43 ,44 ,45 ,46 ,47 ,48 ,49 ,50 ,51 ,52 ,53 ,54 ,55 ,56 ,57 ,58 ,59 ,60 ,61 ,62 ,63 ,64 ,65 ,66 ,67 ,68 ,69 ,70 ,71 ,72 ,73 ,74 ,75 ,76 ,77 ,78 ,79 ,80 ,81 ,82 ,83 ,84 ,85 ,86 ,87 ,88 ,89 ,90 ,91 ,92 ,93 ,94 ,95 ,96 ,97 ,98 ,99 ,100 ,101 ,102 ,103 ,104 ,105 ,106 ,107 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,120 ,121 ,122 ,123 ,124 ,125 ,126 ,127 ,128 ,129 ,130 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,146 ,147 ,148 ,149 ,150 ,151 ,152 ,153 ,154 ,155 ,156 ,157 ,158 ,159 ,160 ,161 ,162 ,163 ,164 ,165 ,166 ,167 ,168 ,169 ,170 ,171 ,172 ,173 ,174 ,175 ,176 ,177 ,178 ,179 ,180 ,181 ,182 ,183 ,184 ,185 ,186 ,187 ,188 ,189 ,190 ,191 ,192 ,193 ,194 ,195 ,196 ,197 ,198 ,199 ,200 ,201 ,202 ,203 ,204 ,205 ,206 ,207 ,208 ,209 ,210 ,211 ,212 ,213 ,214 ,215 ,216 ,217 ,218 ,219 ,220 ,221 ,222 ,223 ,224 ,225 ,226 ,227 ,228 ,229 ,230 ,231 ,232 ,233 ,234 ,235 ,236 ,237 ,238 ,239 ,240 ,241 ,242 ,243 ,244 ,245 ,246 ,247 ,248 ,249 ,250 ,251 ,252 ,253 ,254 ,255 ,256 ,257 ,258 ,259 ,260 ,261 ,262 ,263 ,264 ,265 ,266 ,267 ,268 ,269 ,270 ,271 ,272 ,273 ,274 ,275 ,276 ,277 ,278 ,279 ,280 ,281 ,282 ,283 ,284 ,285 ,286 ,287 ,288 ,289 ,290 ,291 ,292 ,293 ,294 ,295 ,296 ,297 ,298 ,299 ,300 ,301 ,302 ,303 ,304 ,305 ,306 ,307 ,308 ,309 ,310 ,311 ,312 ,313 ,314 ,315 ,316 ,317 ,318 ,319 ,320 ,321 ,322 ,323 ,324 ,325 ,326 ,327 ,328 ,329 ,330 ,331 ,332 ,333 ,334 ,335 ,336 ,337 ,338 ,339 ,340 ,341 ,342 ,343 ,344 ,345 ,346 ,347 ,348 ,349 ,350 ,351 ,352 ,353 ,354 ,355 ,356 ,357 ,358 ,359 ,360 ,361 ,362 ,363 ,364 ,365 ,366 ,367 ,368 ,369 ,370 ,371 ,372 ,373 ,374 ,375 ,376 ,377 ,378 ,379 ,380 ,381 ,382 ,383 ,384 ,385 ,386 ,387 ,388 ,389 ,390 ,391 ,392 ,393 ,394 ,395 ,396 ,397 ,398 ,399 ,400 ,401 ,402 ,403 ,404 ,405 ,406 ,407 ,408 ,409 ,410 ,411 ,412 ,413 ,414 ,415 ,416 ,417 ,418 ,419 ,420 ,421 ,422 ,423 ,424 ,425 ,426 ,427 ,428 ,429 ,430 ,431 ,432 ,433 ,434 ,435 ,436 ,437 ,438 ,439 ,440 ,441 ,442 ,443 ,444 ,445 ,446 ,447 ,448 ,449 ,450 ,451 ,452 ,453 ,454 ,455 ,456 ,457 ,458 ,459 ,460 ,461 ,462 ,463 ,464 ,465 ,466 ,467 ,468 ,469 ,470 ,471 ,472 ,473 ,474 ,475 ,476 ,477 ,478 ,479 ,480 ,481 ,482 ,483 ,484 ,485 ,486 ,487 ,488 ,489 ,490 ,491 ,492 ,493 ,494 ,495 ,496 ,497 ,498 ,499 ,"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4Qz1sSpemXIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Applying Kneyser Ney to the language model"
      ],
      "metadata": {
        "id": "hFg73c_onI0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formula"
      ],
      "metadata": {
        "id": "ZVXbI0MVoqAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "P(traigram) = Ckn + lamda*Pcontintution(Wi)\n",
        "\n",
        "Ckn = max((frequency of triagram)-d,0)/frequency of bigram \n",
        "\n",
        "lamda = d/C(wi-1)*|no of preceeding words|\n",
        "\n",
        "C(wi-1) = frequency of semi final word\n",
        "\n",
        "Pcont(wi) = count(final word)/len(sentences)"
      ],
      "metadata": {
        "id": "Q45sQ8_aouoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model ={}\n",
        "for i in range(len(new_sentences)):\n",
        "  sent = new_sentences[i][:-1]\n",
        "  doc = nlp(sent)\n",
        "  tokens=[token.text for token in doc]\n",
        "  #GENERATING EACH SENTENCE TRIAGRAMS and FINDING PROBABILITIES USING KNEYSER NEY SMOOTHING\n",
        "  probability={}\n",
        "  for j in range(3,len(tokens)+1):\n",
        "    key = tokens[j-3]+\" \"+tokens[j-2]+\" \"+tokens[j-1]\n",
        "    probability[key]=kneyserNey(tokens[j-3:j])\n",
        "\n",
        "  sent_prob=1\n",
        "  for probs in probability:\n",
        "    sent_prob *=probability[probs]\n",
        "  model[sentences[i]]=sent_prob\n",
        "\n",
        "#find average probability\n",
        "total_prob = 0\n",
        "for sent_prob in model:\n",
        "  total_prob += model[sent_prob]\n",
        "avg_prob= total_prob/len(new_sentences)\n",
        "\n",
        "print(avg_prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChnrxD7JpBMV",
        "outputId": "61bccdba-a8e4-4596-b8d7-13520398c1ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0012502094357660152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing after applying smoothing"
      ],
      "metadata": {
        "id": "luPMzrEjsFvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=input(\"input sentence : \")\n",
        "#print(model[sentence+\"\\n\"])\n",
        "act_sent=sentence\n",
        "sentence = re.sub(r'[^\\w\\s]', '',sentence)\n",
        "doc = nlp(sentence)\n",
        "tokens=[token.text for token in doc]\n",
        "probes={}\n",
        "\n",
        "for j in range(3,len(tokens)+1):\n",
        "    key = tokens[j-3]+\" \"+tokens[j-2]+\" \"+tokens[j-1]\n",
        "    probes[key]=kneyserNey(tokens[j-3:j])\n",
        "sent_prob=1\n",
        "for probs in probes:\n",
        "  sent_prob *=probes[probs]\n",
        "print(sent_prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQtKjiNJquvr",
        "outputId": "53a29c09-f145-4922-c9ae-b26731561afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input sentence : In these circumstances, the precautionary principle must be adopted to the full.\n",
            "5.618609574410502e-07\n"
          ]
        }
      ]
    }
  ]
}